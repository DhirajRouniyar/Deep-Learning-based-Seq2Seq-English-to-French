{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9a87ca",
   "metadata": {},
   "source": [
    " # Question 2 - sub question 1, 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9a9820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.02664298191666603\n",
      "Epoch 2/10, Loss: 0.004362978041172028\n",
      "Epoch 3/10, Loss: 0.00336488988250494\n",
      "Epoch 4/10, Loss: 0.002841379027813673\n",
      "Epoch 5/10, Loss: 0.0024992539547383785\n",
      "Epoch 6/10, Loss: 0.002202842151746154\n",
      "Epoch 7/10, Loss: 0.0019441695185378194\n",
      "Epoch 8/10, Loss: 0.0017336936434730887\n",
      "Epoch 9/10, Loss: 0.0015591580886393785\n",
      "Epoch 10/10, Loss: 0.001412449637427926\n",
      "Training MSE: 0.0011888565495610237\n",
      "Test MSE: 0.002450475934892893\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# load training and test data\n",
    "def loadData():\n",
    "    X_train = np.load('X_train.npy',allow_pickle=True)\n",
    "    y_train = np.load('y_train.npy',allow_pickle=True)\n",
    "    X_test = np.load('X_test.npy',allow_pickle=True)\n",
    "    y_test = np.load('y_test.npy',allow_pickle=True)\n",
    "\n",
    "    X_train = [torch.Tensor(x) for x in X_train]  # List of Tensors (SEQ_LEN[i],INPUT_DIM) i=0..NUM_SAMPLES-1\n",
    "    X_test = [torch.Tensor(x) for x in X_test]  # List of Tensors (SEQ_LEN[i],INPUT_DIM)\n",
    "    y_train = torch.Tensor(y_train) # (NUM_SAMPLES,1)\n",
    "    y_test = torch.Tensor(y_test) # (NUM_SAMPLES,1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Define a Vanilla RNN layer by hand\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = self.W_xh(x) + self.W_hh(hidden)\n",
    "        hidden = self.activation(combined)\n",
    "        return hidden\n",
    "\n",
    "# Define a sequence prediction model using the Vanilla RNN\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = RNNLayer(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, seq_lengths):\n",
    "        batch_size = len(input_seq)\n",
    "        last_hidden = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            hidden = torch.zeros(1, self.hidden_size).to(device)\n",
    "\n",
    "            seq_length = seq_lengths[b]\n",
    "            for t in range(seq_length):\n",
    "                hidden = self.rnn(input_seq[b][t], hidden)\n",
    "\n",
    "            # Store the last hidden state in the output tensor\n",
    "            last_hidden[b] = hidden\n",
    "\n",
    "        output = self.linear(last_hidden)\n",
    "        return output\n",
    "\n",
    "# Define hyperparameters and other settings\n",
    "input_size = 10  \n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Load data \n",
    "X_train, X_test, y_train, y_test = loadData()\n",
    "device = y_train.device\n",
    "\n",
    "# Create the model using min length input\n",
    "seq_lengths = [seq.shape[0] for seq in X_train]\n",
    "\n",
    "# Create the model\n",
    "model = SequenceModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Training loop\n",
    "def train(model, num_epochs, lr, batch_size, X_train, y_train, seq_lengths):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            inputs = X_train[i:i+batch_size]\n",
    "            targets = y_train[i:i+batch_size]\n",
    "            lengths = seq_lengths[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# initialize and train Vanilla RNN\n",
    "trained_model = train(model, num_epochs, learning_rate, batch_size, X_train, y_train, seq_lengths)\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X, [seq.shape[0] for seq in X])\n",
    "    mse = nn.MSELoss()(predictions, y)\n",
    "    return mse.item()\n",
    "\n",
    "train_mse = evaluate(trained_model, X_train, y_train)\n",
    "test_mse = evaluate(trained_model, X_test, y_test)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b7426",
   "metadata": {},
   "source": [
    "# Question 2 - sub question 2, 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6565dc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.009028473868966103\n",
      "Epoch [2/10], Loss: 0.009373912587761879\n",
      "Epoch [3/10], Loss: 0.008757087402045727\n",
      "Epoch [4/10], Loss: 0.008849642239511013\n",
      "Epoch [5/10], Loss: 0.008843314833939075\n",
      "Epoch [6/10], Loss: 0.008863409049808979\n",
      "Epoch [7/10], Loss: 0.00888094399124384\n",
      "Epoch [8/10], Loss: 0.008892207406461239\n",
      "Epoch [9/10], Loss: 0.008898316882550716\n",
      "Epoch [10/10], Loss: 0.008899688720703125\n",
      "Training MSE: 0.009202002547681332\n",
      "Test MSE: 0.009862154722213745\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Load training and test data\n",
    "def loadData():\n",
    "    X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "    y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "    X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "    y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "\n",
    "    X_train = [torch.Tensor(x) for x in X_train]\n",
    "    X_test = [torch.Tensor(x) for x in X_test]\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Defining a simple RNN-based sequence prediction model with fixed time steps\n",
    "class SequenceModelFixedTimeSteps(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seq_length):\n",
    "        super(SequenceModelFixedTimeSteps, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters and other settings\n",
    "input_dim = 10  \n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = loadData()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Determine the minimum sequence length\n",
    "min_seq_length = min(len(seq) for seq in X_train)\n",
    "\n",
    "# Trim or pad sequences to the minimum length\n",
    "X_train = [seq[:min_seq_length] for seq in X_train]\n",
    "X_test = [seq[:min_seq_length] for seq in X_test]\n",
    "\n",
    "# Convert the data to tensors and create dataloaders\n",
    "X_train = torch.stack(X_train)\n",
    "X_test = torch.stack(X_test)\n",
    "y_train = y_train.view(-1, 1)  # Ensure the correct shape for the labels\n",
    "\n",
    "# Initialize the model\n",
    "model = SequenceModelFixedTimeSteps(input_dim, hidden_dim, output_dim, min_seq_length).to(device)\n",
    "\n",
    "# Training loop\n",
    "def train(model, num_epochs, lr, batch_size, X_train, y_train):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            inputs = X_train[i:i+batch_size]\n",
    "            targets = y_train[i:i+batch_size]\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "# initialize and train Sequential NN fixing #timesteps to the minimum sequence length\n",
    "trained_model = train(model, num_epochs, learning_rate, batch_size, X_train, y_train)\n",
    "\n",
    "# -----  Question 2_Sub question 5 -----#\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X)\n",
    "    mse = nn.MSELoss()(predictions, y)\n",
    "    return mse.item()\n",
    "\n",
    "train_mse = evaluate(trained_model, X_train, y_train)\n",
    "test_mse = evaluate(trained_model, X_test, y_test)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503420a",
   "metadata": {},
   "source": [
    "# Question 2 - sub question 3, 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa51547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.014521529898047447\n",
      "Epoch [2/10], Loss: 0.007491786032915115\n",
      "Epoch [3/10], Loss: 0.006498521659523249\n",
      "Epoch [4/10], Loss: 0.006131777539849281\n",
      "Epoch [5/10], Loss: 0.005994045175611973\n",
      "Epoch [6/10], Loss: 0.006073987111449242\n",
      "Epoch [7/10], Loss: 0.0072938185185194016\n",
      "Epoch [8/10], Loss: 0.007871345616877079\n",
      "Epoch [9/10], Loss: 0.007722165901213884\n",
      "Epoch [10/10], Loss: 0.007495713885873556\n",
      "Training MSE: 0.00991019792854786\n",
      "Test MSE: 0.011621176265180111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Load training and test data\n",
    "def loadData():\n",
    "    X_train = np.load('X_train.npy',allow_pickle=True)\n",
    "    y_train = np.load('y_train.npy',allow_pickle=True)\n",
    "    X_test = np.load('X_test.npy',allow_pickle=True)\n",
    "    y_test = np.load('y_test.npy',allow_pickle=True)\n",
    "\n",
    "    X_train = [torch.Tensor(x) for x in X_train]  # List of Tensors (SEQ_LEN[i],INPUT_DIM) i=0..NUM_SAMPLES-1\n",
    "    X_test = [torch.Tensor(x) for x in X_test]  # List of Tensors (SEQ_LEN[i],INPUT_DIM)\n",
    "    y_train = torch.Tensor(y_train) # (NUM_SAMPLES,1)\n",
    "    y_test = torch.Tensor(y_test) # (NUM_SAMPLES,1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Define a Vanilla RNN layer by hand\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden = self.activation(self.W_xh(x) + self.W_hh(hidden))\n",
    "        return hidden\n",
    "\n",
    "# Define a sequence prediction model for variable length sequences, NO SHARED WEIGHTS\n",
    "class SequenceModelVarLen(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceModelVarLen, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_layer = RNNLayer(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, seq_lengths):\n",
    "        batch_size = input_seq.size(0)\n",
    "        max_seq_length = input_seq.size(1)\n",
    "        \n",
    "        # Initialize hidden state for each sequence in the batch\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(input_seq.device)\n",
    "\n",
    "        for t in range(max_seq_length):\n",
    "            # Apply RNN layer at each time step\n",
    "            hidden = self.rnn_layer(input_seq[:, t, :], hidden)\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "\n",
    "# Define hyperparameters and other settings\n",
    "input_size = 10  \n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = loadData()\n",
    "device = X_train[0].device\n",
    "\n",
    "# Create the model\n",
    "model = SequenceModelVarLen(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Training loop\n",
    "def train(model, num_epochs, lr, batch_size, X_train, y_train):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            inputs = X_train[i:i+batch_size]\n",
    "            targets = y_train[i:i+batch_size]\n",
    "            \n",
    "            # Pad sequences to the same length (maximum sequence length in the batch)\n",
    "            max_seq_length = max([seq.size(0) for seq in inputs])\n",
    "            padded_inputs = torch.zeros(batch_size, max_seq_length, input_size).to(device)\n",
    "            for j, seq in enumerate(inputs):\n",
    "                seq_len = seq.size(0)\n",
    "                padded_inputs[j, :seq_len, :] = seq\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(padded_inputs, None)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "# initialize and train Sequential NN fixing #timesteps to the maximum sequence length\n",
    "trained_model = train(model, num_epochs, learning_rate, batch_size, X_train, y_train)\n",
    "\n",
    "# -----  Question 2_Sub question 5 -----#\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse_sum = 0.0\n",
    "        for i in range(len(X)):\n",
    "            max_seq_length = X[i].size(0)\n",
    "            outputs = model(X[i].unsqueeze(0), [max_seq_length])\n",
    "            mse_sum += nn.MSELoss()(outputs, y[i:i+1])  # Calculate MSE for each sequence individually\n",
    "        mse = mse_sum / len(X)\n",
    "    return mse.item()\n",
    "\n",
    "train_mse = evaluate(trained_model, X_train, y_train)\n",
    "test_mse = evaluate(trained_model, X_test, y_test)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66512dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
